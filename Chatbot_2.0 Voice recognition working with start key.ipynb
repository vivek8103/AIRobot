{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a6c8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk, random, json , pickle\n",
    "nltk.download('punkt');nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import flatten\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation,Dropout\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af54a456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  >>> import nltk\n",
    "  >>> nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "779b5097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 842ms/step - loss: 2.3497 - accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.5670 - accuracy: 1.0000\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.6299 - accuracy: 1.0000\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0315 - accuracy: 1.0000\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.5590 - accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3969 - accuracy: 1.0000\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3097 - accuracy: 1.0000\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7905 - accuracy: 1.0000\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0571 - accuracy: 1.0000\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.6861e-05 - accuracy: 1.0000\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0951 - accuracy: 1.0000\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.0034e-04 - accuracy: 1.0000\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7687 - accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.3446e-07 - accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2995 - accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.4654e-04 - accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.8876e-06 - accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2444 - accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.9165e-05 - accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.5034e-06 - accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.9605e-07 - accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4568 - accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.8265e-05 - accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7937e-04 - accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5508e-04 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0266e-06 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.3644e-06 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0027e-05 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0532 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.4080e-05 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.5103e-04 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.3113e-06 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.5497e-06 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.8412e-06 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.9605e-07 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.3379e-06 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0266e-06 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.9605e-07 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.5763e-06 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.7206e-05 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.7684e-07 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.8110e-04 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.9605e-07 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.5497e-06 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0398e-05 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0431 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.3611e-04 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.1526e-07 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.5763e-07 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2650e-06 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.9605e-07 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.7684e-07 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8500e-04 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.1723e-06 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4305e-06 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.0942e-04 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.2411e-05 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.5391e-05 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.4305e-06 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.7949e-06 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.5763e-07 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.5565e-06 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.7510e-04 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.5401e-04 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2337e-04 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9024e-04 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7881e-06 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2409e-04 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.7806e-04 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2173e-05 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4305e-06 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.5736e-05 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.5763e-06 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "class Training:\n",
    "    def __init__(self):\n",
    "    #read and load the intent file\n",
    "        data_file=open('intents.json').read()\n",
    "        self.intents=json.loads(data_file)['intents']\n",
    "        self.ignore_words=list(\"!@#$%^&*?\")\n",
    "        self.process_data()\n",
    "        \n",
    "        #run window\n",
    "    def run(self):\n",
    "        self.process_data()\n",
    "        self.train_data()\n",
    "        self.build()\n",
    "        \n",
    "        \n",
    "    def process_data(self):\n",
    "    #fetch patterns and tokenize them into words\n",
    "        self.pattern=list(map(lambda x:x[\"patterns\"],self.intents))\n",
    "        self.words=list(map(word_tokenize,flatten(self.pattern)))\n",
    "    #fetch classes i.e. tags and store in documents along with tokenized patterns\n",
    "        self.classes= flatten( [[x[\"tag\"]]*len(y) for x,y in zip(self.intents,self.pattern)])\n",
    "        self.documents=list(map(lambda x,y:(x,y),self.words,self.classes))\n",
    "    #lower case and filter all the symbols from words\n",
    "        self.words=list(map(str.lower,flatten(self.words)))\n",
    "        self.words=list(filter(lambda x:x not in self.ignore_words,self.words))\n",
    "\n",
    "    #lemmatize the words and sort the class and word lists\n",
    "        self.words=list(map(lemmatizer.lemmatize,self.words))\n",
    "        self.words=sorted(list(set(self.words)))\n",
    "        self.classes=sorted(list(set(self.classes)))\n",
    "        \n",
    "    def train_data(self):\n",
    "        #initialize and set analyzer=word as we want to vectorize words not characters\n",
    "        cv=CountVectorizer(tokenizer=lambda txt: \n",
    "        txt.split(),analyzer=\"word\",stop_words=None)\n",
    "        #create the training sets for model\n",
    "        training=[]\n",
    "        for doc in self.documents:\n",
    "        #lower case and lemmatize the pattern words\n",
    "            pattern_words=list(map(str.lower,doc[0]))\n",
    "            pattern_words=' '.join(list(map(lemmatizer.lemmatize,pattern_words)))\n",
    "\n",
    "        #train or fit the vectorizer with all words\n",
    "        #and transform into one-hot encoded vector\n",
    "        vectorize=cv.fit([' '.join(self.words)])\n",
    "        word_vector=vectorize.transform([pattern_words]).toarray().tolist()[0]\n",
    "\n",
    "        #create output for the respective input\n",
    "        #output size will be equal to total numbers of classes\n",
    "        output_row=[0]*len(self.classes)\n",
    "\n",
    "        #if the pattern is from current class put 1 in list else 0\n",
    "        output_row[self.classes.index(doc[1])]=1\n",
    "        cvop=cv.fit([' '.join(self.classes)])\n",
    "        out_p=cvop.transform([doc[1]]).toarray().tolist()[0]\n",
    "\n",
    "        #store vectorized word list long with its class\n",
    "        training.append([word_vector,output_row])\n",
    "\n",
    "        #shuffle training sets to avoid model to train on same data again\n",
    "        random.shuffle(training)\n",
    "        training=np.array(training,dtype=object)\n",
    "        train_x=list(training[:,0]) #patterns\n",
    "        train_y=list(training[:,1]) #classes\n",
    "        return train_x,train_y\n",
    "    \n",
    "    def build(self):\n",
    "        #load the data from train_data function\n",
    "        train_x,train_y = self.train_data()\n",
    "\n",
    "        ##Create a Sequential model with 3 layers.\n",
    "        model=Sequential()\n",
    "        #input layer with latent dimension of 128 neurons and ReLU activation function\n",
    "        model.add(Dense(128,input_shape=(len(train_x[0]),),activation='relu'))\n",
    "        model.add(Dropout(0.5)) #Dropout to avoid overfitting\n",
    "        #second layer with the latent dimension of 64 neurons\n",
    "        model.add(Dense(64,activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        #fully connected output layer with softmax activation function\n",
    "        model.add(Dense(len(train_y[0]),activation='softmax'))\n",
    "        '''Compile model with Stochastic Gradient Descent with learning rate and\n",
    "        nesterov accelerated gradient descent'''\n",
    "        sgd=SGD(lr=1e-2,decay=1e-6,momentum=0.9,nesterov=True)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "        optimizer=sgd,metrics=['accuracy'])\n",
    "        #fit the model with training input and output sets\n",
    "        hist=model.fit(np.array(train_x),np.array(train_y),\n",
    "        epochs=200,batch_size=10,verbose=1)\n",
    "        \n",
    "        #save model and words,classes which can be used for prediction.\n",
    "        model.save('chatbot_model.h5',hist)\n",
    "        pickle.dump({'words':self.words,'classes':self.classes,\n",
    "        'train_x':train_x,'train_y':train_y},open(\"training_data\",\"wb\"))\n",
    "        \n",
    "        \n",
    "\n",
    "# run the file\n",
    "if __name__==\"__main__\":\n",
    "    bot = Training()\n",
    "    bot.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb0e04c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk, random, json , pickle\n",
    "#nltk.download('punkt');nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "202fd258",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "context={};\n",
    "class Testing:\n",
    "    def __init__(self):\n",
    "        #load the intent file\n",
    "        self.intents = json.loads(open('intents.json').read())\n",
    "        #load the training_data file which contains training.py file data\n",
    "        data=pickle.load(open(\"training_data\",\"rb\"))\n",
    "        self.words=data['words']\n",
    "        self.classes=data['classes']\n",
    "        self.model=load_model('chatbot_model.h5')\n",
    "        #set the error threshold value\n",
    "        self.ERROR_THRESHOLD=0.5\n",
    "        self.ignore_words=list(\"!@#$%^&*?\")\n",
    "        \n",
    "    \n",
    "    def clean_up_sentence(self,sentence):\n",
    "        #tokenize each sentence (user's query)\n",
    "        sentence_words=word_tokenize(sentence.lower())\n",
    "        #lemmatize the word to root word and filter symbols words\n",
    "        sentence_words=list(map(lemmatizer.lemmatize,sentence_words))\n",
    "        sentence_words=list(filter(lambda x:x not in \n",
    "        self.ignore_words,sentence_words))\n",
    "        return set(sentence_words)\n",
    "\n",
    "    def wordvector(self,sentence):\n",
    "        #initialize CountVectorizer\n",
    "        #txt.split helps to tokenize single character\n",
    "        cv=CountVectorizer(tokenizer=lambda txt: txt.split())\n",
    "        sentence_words=' '.join(self.clean_up_sentence(sentence))\n",
    "        words=' '.join(self.words)\n",
    "\n",
    "        #fit the words into cv and transform into one-hot encoded vector\n",
    "        vectorize=cv.fit([words])\n",
    "        word_vector=vectorize.transform([sentence_words]).toarray().tolist()[0]\n",
    "        return(np.array(word_vector))\n",
    "    \n",
    "    def classify(self,sentence):\n",
    "        #predict to which class(tag) user's query belongs to\n",
    "        results=self.model.predict(np.array([self.wordvector(sentence)]))[0]\n",
    "        #store the class name and probability of that class\n",
    "        results = list(map(lambda x: [x[0],x[1]], enumerate(results)))\n",
    "        #accept those class probability which are greater then threshold value,0.5\n",
    "        results = list(filter(lambda x: x[1]>self.ERROR_THRESHOLD ,results))\n",
    "\n",
    "        #sort class probability value in descending order\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return_list = []\n",
    "\n",
    "        for i in results:\n",
    "            return_list.append((self.classes[i[0]],str(i[1])))\n",
    "        return return_list\n",
    "    \n",
    "    def results(self,sentence,userID):\n",
    "        #if context is maintained then filter class(tag) accordingly\n",
    "        if sentence.isdecimal():\n",
    "            if context[userID]==\"historydetails\":\n",
    "                return self.classify('ordernumber')\n",
    "        return self.classify(sentence)\n",
    "    \n",
    "    def response(self,sentence,userID='TechVidvan'):\n",
    "        #get class of users query\n",
    "        results=self.results(sentence,userID)\n",
    "        print(sentence,results)\n",
    "        #store random response to the query\n",
    "        ans=\"\"\n",
    "        if results:\n",
    "            while results:\n",
    "                for i in self.intents['intents']:\n",
    "                #check if tag == query's class\n",
    "                    if i['tag'] == results[0][0]:\n",
    "\n",
    "                   #if class contains key as \"set\"\n",
    "                   #then store key as userid along with its value in\n",
    "                   #context dictionary\n",
    "                        if 'set' in i and not 'filter' in i:\n",
    "                            context[userID] = i['set']\n",
    "                    #if the tag doesn't have any filter return response\n",
    "                        if not 'filter' in i:\n",
    "                            ans=random.choice(i['responses'])\n",
    "\n",
    "                       #if a class has key as filter then check if context dictionary key's value is same as filter value\n",
    "                       #return the random response\n",
    "                        if userID in context and 'filter' in i and i['filter']==context[userID]:\n",
    "                            if 'set' in i:\n",
    "                                context[userID] = i['set']\n",
    "                        ans=random.choice(i['responses'])\n",
    "\n",
    "        results.pop(0)\n",
    "        #if ans contains some value then return response to user's query else return some message\n",
    "        return ans if ans!=\"\" else \"Sorry ! I am still Learning.\\nYou can train me by providing more datas.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dee7d60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\users\\vivek\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: gTTS in c:\\users\\vivek\\anaconda3\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: six in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from gTTS) (1.16.0)\n",
      "Requirement already satisfied: requests in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from gTTS) (2.27.1)\n",
      "Requirement already satisfied: click in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from gTTS) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from click->gTTS) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from requests->gTTS) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from requests->gTTS) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from requests->gTTS) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from requests->gTTS) (1.26.9)\n",
      "Requirement already satisfied: transformers in c:\\users\\vivek\\anaconda3\\lib\\site-packages (4.21.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\vivek\\anaconda3\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vivek\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: pyaudio in c:\\users\\vivek\\anaconda3\\lib\\site-packages (0.2.12)\n"
     ]
    }
   ],
   "source": [
    "# To be able to convert text to Speech\n",
    "! pip install SpeechRecognition  \n",
    "#To convey the Speech to text and also speak it out\n",
    "!pip install gTTS \n",
    "# To install our language model\n",
    "!pip install transformers  \n",
    "!pip install tensorflow \n",
    "!pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9424b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "nlp = transformers.pipeline(\"conversational\", \n",
    "                            model=\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e635a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db0d49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 2ms/step - loss: 2.8804 - accuracy: 0.0455\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 2.8955 - accuracy: 0.0455\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 2.8678 - accuracy: 0.1136\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.7640 - accuracy: 0.0455\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 2.7989 - accuracy: 0.1136 \n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.8055 - accuracy: 0.0455\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.7609 - accuracy: 0.0682\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.7010 - accuracy: 0.0909\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 2.7122 - accuracy: 0.1364 \n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.6201 - accuracy: 0.3182\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.6226 - accuracy: 0.2045\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.6010 - accuracy: 0.1818\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.5025 - accuracy: 0.3182\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.4078 - accuracy: 0.3636\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.4923 - accuracy: 0.2500\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.4911 - accuracy: 0.2500\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.4276 - accuracy: 0.2955\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.2866 - accuracy: 0.4091\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.1724 - accuracy: 0.4091\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.3512 - accuracy: 0.2273\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.2487 - accuracy: 0.2955\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.2174 - accuracy: 0.3409\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.2151 - accuracy: 0.3864\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.2089 - accuracy: 0.2955\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.1223 - accuracy: 0.4318\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 2.0294 - accuracy: 0.4091\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.0098 - accuracy: 0.5227\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.8301 - accuracy: 0.5682\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.8906 - accuracy: 0.5000\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.8451 - accuracy: 0.5455\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.8116 - accuracy: 0.5909\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.7591 - accuracy: 0.5909\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.7105 - accuracy: 0.5455\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 1.5664 - accuracy: 0.6364\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.4925 - accuracy: 0.6136\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 1.4484 - accuracy: 0.5682\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.5585 - accuracy: 0.5227\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.4289 - accuracy: 0.6364\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.3791 - accuracy: 0.6364\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.3439 - accuracy: 0.6364\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.3224 - accuracy: 0.6364\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.1564 - accuracy: 0.7273\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 1.2767 - accuracy: 0.6136\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.2727 - accuracy: 0.6591\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.3370 - accuracy: 0.5682\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.1009 - accuracy: 0.6818\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.1934 - accuracy: 0.5909\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 1.0067 - accuracy: 0.7500\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.8307 - accuracy: 0.7500\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0768 - accuracy: 0.7045\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.0071 - accuracy: 0.7045\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.1171 - accuracy: 0.6818\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.9146 - accuracy: 0.7045\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.1714 - accuracy: 0.6818\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.7950 - accuracy: 0.7955\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.8830 - accuracy: 0.7955\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.8404 - accuracy: 0.7727\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.7117 - accuracy: 0.7955\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5719 - accuracy: 0.8636\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.6630 - accuracy: 0.9091\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.9108 - accuracy: 0.6591\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.6812 - accuracy: 0.8409\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.7322 - accuracy: 0.8409\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.7008 - accuracy: 0.7955\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.6305 - accuracy: 0.7955\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6465 - accuracy: 0.7955\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6288 - accuracy: 0.8636\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6222 - accuracy: 0.8636\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5347 - accuracy: 0.8636\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4835 - accuracy: 0.9318\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.7727\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5986 - accuracy: 0.9091\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3972 - accuracy: 0.9318\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.4683 - accuracy: 0.9318\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.3966 - accuracy: 0.9318\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.4479 - accuracy: 0.9091\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5835 - accuracy: 0.8636\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.6055 - accuracy: 0.8636\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.3688 - accuracy: 0.9318\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.3878 - accuracy: 0.9773\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.5650 - accuracy: 0.8182\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4237 - accuracy: 0.8864\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4411 - accuracy: 0.8636\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.4978 - accuracy: 0.8864\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.3967 - accuracy: 0.9318\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4354 - accuracy: 0.9091\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4015 - accuracy: 0.8864\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5580 - accuracy: 0.8636\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4302 - accuracy: 0.8636\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6391 - accuracy: 0.8182\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4197 - accuracy: 0.9545\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2358 - accuracy: 0.9773\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3404 - accuracy: 0.9091\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2018 - accuracy: 0.9773\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2989 - accuracy: 0.9545\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2454 - accuracy: 0.9773\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3265 - accuracy: 0.9091\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3545 - accuracy: 0.9318\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4187 - accuracy: 0.8409\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2313 - accuracy: 0.9773\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3026 - accuracy: 0.9318\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2700 - accuracy: 0.9545\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2070 - accuracy: 0.9773\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.4252 - accuracy: 0.9091\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.4469 - accuracy: 0.8182\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.3244 - accuracy: 0.9318\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3414 - accuracy: 0.9091\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2658 - accuracy: 0.9545\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2808 - accuracy: 0.8864\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2374 - accuracy: 0.9318\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4378 - accuracy: 0.8182\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3401 - accuracy: 0.9545\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2883 - accuracy: 0.9318\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3064 - accuracy: 0.9773\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2764 - accuracy: 0.9318\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1643 - accuracy: 0.9773\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3537 - accuracy: 0.9318\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1258 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2354 - accuracy: 0.9773\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.9545\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2104 - accuracy: 0.9091\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2194 - accuracy: 0.9545\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2227 - accuracy: 0.9773\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1573 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1978 - accuracy: 0.9545\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1364 - accuracy: 0.9773\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2604 - accuracy: 0.9091\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2866 - accuracy: 0.9091\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1800 - accuracy: 0.9545\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1619 - accuracy: 0.9773\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1600 - accuracy: 0.9773\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1939 - accuracy: 0.9545\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2594 - accuracy: 0.9091\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2688 - accuracy: 0.9545\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1743 - accuracy: 0.9545\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2430 - accuracy: 0.9318\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1222 - accuracy: 0.9773\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1593 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1777 - accuracy: 0.9773\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1326 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1580 - accuracy: 0.9773\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1874 - accuracy: 0.9773\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1657 - accuracy: 0.9773\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2285 - accuracy: 0.9545\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1304 - accuracy: 0.9773\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1555 - accuracy: 0.9773\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2611 - accuracy: 0.9545\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1182 - accuracy: 0.9773\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3290 - accuracy: 0.8864\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1597 - accuracy: 0.9545\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2485 - accuracy: 0.9091\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0946 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1815 - accuracy: 0.9545\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1517 - accuracy: 0.9545\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1224 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1272 - accuracy: 0.9773\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1722 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2426 - accuracy: 0.8864\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2095 - accuracy: 0.9773\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2277 - accuracy: 0.9545\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2468 - accuracy: 0.9773\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1790 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0701 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1763 - accuracy: 0.9773\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1765 - accuracy: 0.9318\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2378 - accuracy: 0.9318\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2299 - accuracy: 0.9545\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2024 - accuracy: 0.9545\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1010 - accuracy: 0.9773\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2448 - accuracy: 0.9545\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2416 - accuracy: 0.9318\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2097 - accuracy: 0.9545\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1658 - accuracy: 0.9318\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1774 - accuracy: 0.9091\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.0377 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1490 - accuracy: 0.9773\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2083 - accuracy: 0.9545\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1482 - accuracy: 0.9545\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1262 - accuracy: 0.9773\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1526 - accuracy: 0.9773\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.9773\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0759 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1258 - accuracy: 0.9773\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1156 - accuracy: 0.9545\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0886 - accuracy: 0.9545\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1644 - accuracy: 0.9545\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1412 - accuracy: 0.9545\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.0912 - accuracy: 0.9773\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1412 - accuracy: 0.9545\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1308 - accuracy: 0.9773\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1844 - accuracy: 0.9773\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2618 - accuracy: 0.9318\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2088 - accuracy: 0.9545\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0863 - accuracy: 0.9773\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1317 - accuracy: 0.9545\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1182 - accuracy: 0.9545\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1132 - accuracy: 0.9773\n",
      "say anything : \n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "hello my how are you [('botstatus', '0.68206644')]\n",
      "Query: hello my how are you\n",
      "Bot: I am fine.Thanks for asking.\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "hello my how are you [('botstatus', '0.68206644')]\n",
      "Query: hello my how are you\n",
      "Bot: I am fine.Thanks for asking.\n",
      "ai -->  I am fine.Thanks for asking.\n",
      "say anything : \n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "good morning how are you [('botstatus', '0.6251481')]\n",
      "Query: good morning how are you\n",
      "Bot: I am doing good.\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good morning how are you [('botstatus', '0.6251481')]\n",
      "Query: good morning how are you\n",
      "Bot: I am fine.Thanks for asking.\n",
      "ai -->  I am fine.Thanks for asking.\n",
      "say anything : \n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "how is the weather today [('Weather', '0.9997626')]\n",
      "Query: how is the weather today\n",
      "Bot: Its sunny outside\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how is the weather today [('Weather', '0.9997626')]\n",
      "Query: how is the weather today\n",
      "Bot: No it isnt raining today\n",
      "ai -->  No it isnt raining today\n",
      "say anything : \n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "import json\n",
    "#import the training.py\n",
    "#and testing.py file\n",
    "import testing as testpy\n",
    "import training as trainpy\n",
    "\n",
    "\n",
    "from threading import *\n",
    "\n",
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "BG_GRAY=\"#ABB2B9\"\n",
    "BG_COLOR=\"#000\"\n",
    "TEXT_COLOR=\"#FFF\"\n",
    "FONT=\"Helvetica 14\"\n",
    "FONT_BOLD=\"Helvetica 13 bold\"\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        #initialize tkinter window\n",
    "        self.window=Tk()\n",
    "        self.main_window()\n",
    "        self.test=testpy.Testing()\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def speech_to_text(self):\n",
    "        recognizer = sr.Recognizer()\n",
    "        with sr.Microphone() as source:\n",
    "            recognizer.adjust_for_ambient_noise(source, duration=1)           \n",
    "            # r.energy_threshold()\n",
    "            print(\"say anything : \")\n",
    " \n",
    "            audio = recognizer.listen(source, phrase_time_limit = 5)\n",
    "            try:\n",
    "                self.text = recognizer.recognize_google(audio)\n",
    "                return(self.text)\n",
    "            except:\n",
    "                self.work()\n",
    "    \n",
    "    def wake_up(self, text):\n",
    "        return True if self.name in text.lower() else False\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def text_to_speech(text):\n",
    "        print(\"ai --> \", text)\n",
    "        speaker = gTTS(text=text, lang=\"en\", slow=False)\n",
    "        speaker.save(\"res.mp3\")\n",
    "        os.system(\"start res.mp3\")  #macbook->afplay | windows->start\n",
    "        time.sleep(6)\n",
    "        os.remove(\"res.mp3\")\n",
    "\n",
    "    @staticmethod\n",
    "    def action_time():\n",
    "        return datetime.datetime.now().time().strftime('%H:%M')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #run window\n",
    "    def run(self):\n",
    "        self.window.mainloop()\n",
    "    \n",
    "    def main_window(self):\n",
    "        #add title to window and configure it\n",
    "        self.window.title(\"ChatBot\")\n",
    "        self.window.resizable(width=False,height=False)\n",
    "        self.window.configure(width=520,height=520,bg=BG_COLOR)\n",
    "        #add tab for Chatbot and Train Bot in Notebook frame\n",
    "        self.tab = ttk.Notebook(self.window)\n",
    "        self.tab.pack(expand=1,fill='both')\n",
    "        self.bot_frame=ttk.Frame(self.tab,width=520,height=520)\n",
    "        self.train_frame=ttk.Frame(self.tab,width=520,height=520)\n",
    "        self.tab.add(self.bot_frame,text='TechVantageBot'.center(100))\n",
    "        self.tab.add(self.train_frame,text='Train Bot'.center(100))\n",
    "        self.add_bot()\n",
    "        self.add_train()\n",
    "        \n",
    "\n",
    "        \n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    def add_bot(self):\n",
    "        #Add heading to the Chabot window\n",
    "        head_label=Label(self.bot_frame,bg=BG_COLOR,fg=TEXT_COLOR,text=\"Welcome to TechVantage\",font=FONT_BOLD,pady=10)\n",
    "        head_label.place(relwidth=1)\n",
    "        line = Label(self.bot_frame,width=450,bg=BG_COLOR)\n",
    "        line.place(relwidth=1,rely=0.07,relheight=0.012)\n",
    "\n",
    "        #create text widget where conversation will be displayed\n",
    "        self.text_widget=Text(self.bot_frame,width=20,height=2,bg=\"#fff\",fg=\"#000\",font=FONT,padx=5,pady=5)\n",
    "        self.text_widget.place(relheight=0.745,relwidth=1,rely=0.08)\n",
    "        self.text_widget.configure(cursor=\"arrow\",state=DISABLED)\n",
    "\n",
    "        #create scrollbar\n",
    "        scrollbar=Scrollbar(self.text_widget)\n",
    "        scrollbar.place(relheight=1,relx=0.974)\n",
    "        scrollbar.configure(command=self.text_widget.yview)\n",
    "\n",
    "        #create bottom label where message widget will placed\n",
    "        bottom_label=Label(self.bot_frame,bg=BG_GRAY,height=80)\n",
    "        bottom_label.place(relwidth=1,rely=0.825)\n",
    "        #this is for user to put query\n",
    "        self.msg_entry=Entry(bottom_label,bg=\"#2C3E50\",fg=TEXT_COLOR,font=FONT)\n",
    "        self.msg_entry.place(relwidth=0.788,relheight=0.06,rely=0.008,relx=0.008)\n",
    "        self.msg_entry.focus()\n",
    "        self.msg_entry.bind(\"<Return>\",self.on_enter)\n",
    "        #send button which will call on_enter function to send the query\n",
    "        \n",
    "\n",
    "        \n",
    "        send_button=Button(bottom_label,text=\"Start\",font=FONT_BOLD,width=8,bg=\"Green\",command=lambda: self.on_enter(None))   \n",
    "        send_button.place(relx=0.80,rely=0.008,relheight=0.06,relwidth=0.20)\n",
    "\n",
    "    def add_train(self):\n",
    "        #Add heading to the Train Bot window\n",
    "        head_label=Label(self.train_frame,bg=BG_COLOR,fg=TEXT_COLOR,text=\"Train Bot\",font=FONT_BOLD,pady=10)\n",
    "        head_label.place(relwidth=1)\n",
    "\n",
    "        #Tag Label and Entry for intents tag. \n",
    "        taglabel=Label(self.train_frame,fg=\"#000\",text=\"Tag\",font=FONT)\n",
    "        taglabel.place(relwidth=0.2,rely=0.14,relx=0.008)\n",
    "        self.tag=Entry(self.train_frame,bg=\"#fff\",fg=\"#000\",font=FONT)\n",
    "        self.tag.place(relwidth=0.7,relheight=0.06,rely=0.14,relx=0.22)\n",
    "\n",
    "        #Pattern Label and Entry for pattern to our tag.\n",
    "        self.pattern=[]\n",
    "        for i in range(2):\n",
    "            patternlabel=Label(self.train_frame,fg=\"#000\",text=\"Pattern%d\"%(i+1),font=FONT)\n",
    "            patternlabel.place(relwidth=0.2,rely=0.28+0.08*i,relx=0.008)\n",
    "            self.pattern.append(Entry(self.train_frame,bg=\"#fff\",fg=\"#000\",font=FONT))\n",
    "            self.pattern[i].place(relwidth=0.7,relheight=0.06,rely=0.28+0.08*i,relx=0.22)\n",
    "\n",
    "        #Response Label and Entry for response to our pattern.\n",
    "        self.response=[]\n",
    "        for i in range(2):\n",
    "            responselabel=Label(self.train_frame,fg=\"#000\",text=\"Response%d\"%(i+1),font=FONT)\n",
    "            responselabel.place(relwidth=0.2,rely=0.50+0.08*i,relx=0.008)\n",
    "            self.response.append(Entry(self.train_frame,bg=\"#fff\",fg=\"#000\",font=FONT))\n",
    "            self.response[i].place(relwidth=0.7,relheight=0.06,rely=0.50+0.08*i,relx=0.22)\n",
    "\n",
    "        #to train our bot create Train Bot button which will call on_train function\n",
    "        bottom_label=Label(self.train_frame,bg=BG_GRAY,height=80)\n",
    "        bottom_label.place(relwidth=1,rely=0.825)\n",
    "\n",
    "        train_button=Button(bottom_label,text=\"Train Bot\",font=FONT_BOLD,width=12,bg=\"Green\",command=lambda: self.on_train(None))\n",
    "        train_button.place(relx=0.20,rely=0.008,relheight=0.06,relwidth=0.60)\n",
    "    \n",
    "    def on_train(self,event):\n",
    "        #read intent file and append created tag,pattern and responses from add_train function\n",
    "        with open('intents.json','r+') as json_file:\n",
    "            file_data=json.load(json_file)\n",
    "            file_data['intents'].append({\n",
    "            \"tag\": self.tag.get(),\n",
    "            \"patterns\": [i.get() for i in self.pattern],\n",
    "            \"responses\": [i.get() for i in self.response],\n",
    "            \"context\": \"\"\n",
    "            })\n",
    "            json_file.seek(0)\n",
    "            json.dump(file_data, json_file, indent = 1)\n",
    "        #run and compile model from our training.py file.\n",
    "        train=trainpy.Training()\n",
    "        train.build(); print(\"Trained Successfully\")\n",
    "        self.test=testpy.Testing()\n",
    "        \n",
    "        \n",
    "            #Define the function to start the thread   \n",
    "        \n",
    "    def on_enter(self,event):\n",
    "        \n",
    "        # Call work function\n",
    "        t1=Thread(target=self.work)\n",
    "        t1.start()\n",
    "      \n",
    "        # work function\n",
    "    def work(self):\n",
    "        while True:\n",
    "                msg=self.speech_to_text()\n",
    "                self.my_msg(msg,\"You\")\n",
    "                self.bot_response(msg,\"Bot\")\n",
    "        \n",
    "        \n",
    "    def bot_response(self,msg,sender):\n",
    "        self.text_widget.configure(state=NORMAL)\n",
    "        #get the response for the user's query from testing.py file\n",
    "        self.text_widget.insert(END,str(sender)+\" : \"+self.test.response(msg)+\"\\n\\n\")\n",
    "        a = self.test.response(msg)\n",
    "        self.text_to_speech(a)\n",
    "        self.text_widget.configure(state=DISABLED)\n",
    "        self.text_widget.see(END)\n",
    "    \n",
    "    def my_msg(self,msg,sender):\n",
    "        #it will display user query and bot response in text_widget\n",
    "        if not msg:\n",
    "            return\n",
    "        self.msg_entry.delete(0,END)\n",
    "        self.text_widget.configure(state=NORMAL)\n",
    "        self.text_widget.insert(END,str(sender)+\" : \"+str(msg)+\"\\n\")\n",
    "        self.text_widget.configure(state=DISABLED)\n",
    "        \n",
    "# run the file\n",
    "if __name__==\"__main__\":\n",
    "    bot = ChatBot(name=\"maya\")\n",
    "    bot.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfe937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec9d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed152d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
