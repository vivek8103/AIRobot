{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a6c8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk, random, json , pickle\n",
    "nltk.download('punkt');nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import flatten\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation,Dropout\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af54a456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  >>> import nltk\n",
    "  >>> nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "779b5097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 842ms/step - loss: 2.3497 - accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.5670 - accuracy: 1.0000\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.6299 - accuracy: 1.0000\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0315 - accuracy: 1.0000\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.5590 - accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3969 - accuracy: 1.0000\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3097 - accuracy: 1.0000\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7905 - accuracy: 1.0000\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0571 - accuracy: 1.0000\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.6861e-05 - accuracy: 1.0000\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0951 - accuracy: 1.0000\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.0034e-04 - accuracy: 1.0000\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7687 - accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.3446e-07 - accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2995 - accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.4654e-04 - accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.8876e-06 - accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2444 - accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.9165e-05 - accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.5034e-06 - accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.9605e-07 - accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4568 - accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.8265e-05 - accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7937e-04 - accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5508e-04 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0266e-06 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.3644e-06 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0027e-05 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0532 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.4080e-05 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.5103e-04 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.3113e-06 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.5497e-06 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.8412e-06 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.9605e-07 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.3379e-06 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0266e-06 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.9605e-07 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.5763e-06 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.7206e-05 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.7684e-07 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.8110e-04 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.9605e-07 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.5497e-06 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0398e-05 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0431 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.3611e-04 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.1526e-07 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.5763e-07 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2650e-06 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.9605e-07 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.7684e-07 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8500e-04 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.1723e-06 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4305e-06 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.0942e-04 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.2411e-05 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.5391e-05 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.4305e-06 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.7949e-06 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.5763e-07 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.5565e-06 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.7510e-04 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.5401e-04 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2337e-04 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9024e-04 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7881e-06 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2409e-04 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.7806e-04 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2173e-05 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4305e-06 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.5736e-05 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.5763e-06 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "class Training:\n",
    "    def __init__(self):\n",
    "    #read and load the intent file\n",
    "        data_file=open('intents.json').read()\n",
    "        self.intents=json.loads(data_file)['intents']\n",
    "        self.ignore_words=list(\"!@#$%^&*?\")\n",
    "        self.process_data()\n",
    "        \n",
    "        #run window\n",
    "    def run(self):\n",
    "        self.process_data()\n",
    "        self.train_data()\n",
    "        self.build()\n",
    "        \n",
    "        \n",
    "    def process_data(self):\n",
    "    #fetch patterns and tokenize them into words\n",
    "        self.pattern=list(map(lambda x:x[\"patterns\"],self.intents))\n",
    "        self.words=list(map(word_tokenize,flatten(self.pattern)))\n",
    "    #fetch classes i.e. tags and store in documents along with tokenized patterns\n",
    "        self.classes= flatten( [[x[\"tag\"]]*len(y) for x,y in zip(self.intents,self.pattern)])\n",
    "        self.documents=list(map(lambda x,y:(x,y),self.words,self.classes))\n",
    "    #lower case and filter all the symbols from words\n",
    "        self.words=list(map(str.lower,flatten(self.words)))\n",
    "        self.words=list(filter(lambda x:x not in self.ignore_words,self.words))\n",
    "\n",
    "    #lemmatize the words and sort the class and word lists\n",
    "        self.words=list(map(lemmatizer.lemmatize,self.words))\n",
    "        self.words=sorted(list(set(self.words)))\n",
    "        self.classes=sorted(list(set(self.classes)))\n",
    "        \n",
    "    def train_data(self):\n",
    "        #initialize and set analyzer=word as we want to vectorize words not characters\n",
    "        cv=CountVectorizer(tokenizer=lambda txt: \n",
    "        txt.split(),analyzer=\"word\",stop_words=None)\n",
    "        #create the training sets for model\n",
    "        training=[]\n",
    "        for doc in self.documents:\n",
    "        #lower case and lemmatize the pattern words\n",
    "            pattern_words=list(map(str.lower,doc[0]))\n",
    "            pattern_words=' '.join(list(map(lemmatizer.lemmatize,pattern_words)))\n",
    "\n",
    "        #train or fit the vectorizer with all words\n",
    "        #and transform into one-hot encoded vector\n",
    "        vectorize=cv.fit([' '.join(self.words)])\n",
    "        word_vector=vectorize.transform([pattern_words]).toarray().tolist()[0]\n",
    "\n",
    "        #create output for the respective input\n",
    "        #output size will be equal to total numbers of classes\n",
    "        output_row=[0]*len(self.classes)\n",
    "\n",
    "        #if the pattern is from current class put 1 in list else 0\n",
    "        output_row[self.classes.index(doc[1])]=1\n",
    "        cvop=cv.fit([' '.join(self.classes)])\n",
    "        out_p=cvop.transform([doc[1]]).toarray().tolist()[0]\n",
    "\n",
    "        #store vectorized word list long with its class\n",
    "        training.append([word_vector,output_row])\n",
    "\n",
    "        #shuffle training sets to avoid model to train on same data again\n",
    "        random.shuffle(training)\n",
    "        training=np.array(training,dtype=object)\n",
    "        train_x=list(training[:,0]) #patterns\n",
    "        train_y=list(training[:,1]) #classes\n",
    "        return train_x,train_y\n",
    "    \n",
    "    def build(self):\n",
    "        #load the data from train_data function\n",
    "        train_x,train_y = self.train_data()\n",
    "\n",
    "        ##Create a Sequential model with 3 layers.\n",
    "        model=Sequential()\n",
    "        #input layer with latent dimension of 128 neurons and ReLU activation function\n",
    "        model.add(Dense(128,input_shape=(len(train_x[0]),),activation='relu'))\n",
    "        model.add(Dropout(0.5)) #Dropout to avoid overfitting\n",
    "        #second layer with the latent dimension of 64 neurons\n",
    "        model.add(Dense(64,activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        #fully connected output layer with softmax activation function\n",
    "        model.add(Dense(len(train_y[0]),activation='softmax'))\n",
    "        '''Compile model with Stochastic Gradient Descent with learning rate and\n",
    "        nesterov accelerated gradient descent'''\n",
    "        sgd=SGD(lr=1e-2,decay=1e-6,momentum=0.9,nesterov=True)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "        optimizer=sgd,metrics=['accuracy'])\n",
    "        #fit the model with training input and output sets\n",
    "        hist=model.fit(np.array(train_x),np.array(train_y),\n",
    "        epochs=200,batch_size=10,verbose=1)\n",
    "        \n",
    "        #save model and words,classes which can be used for prediction.\n",
    "        model.save('chatbot_model.h5',hist)\n",
    "        pickle.dump({'words':self.words,'classes':self.classes,\n",
    "        'train_x':train_x,'train_y':train_y},open(\"training_data\",\"wb\"))\n",
    "        \n",
    "        \n",
    "\n",
    "# run the file\n",
    "if __name__==\"__main__\":\n",
    "    bot = Training()\n",
    "    bot.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "202fd258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import nltk, random, json , pickle\n",
    "#nltk.download('punkt');nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import transformers\n",
    "nlp = transformers.pipeline(\"conversational\", \n",
    "                            model=\"microsoft/DialoGPT-large\")\n",
    "\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "context={};\n",
    "class Testing:\n",
    "    def __init__(self):\n",
    "        #load the intent file\n",
    "        self.intents = json.loads(open('intents.json').read())\n",
    "        #load the training_data file which contains training.py file data\n",
    "        data=pickle.load(open(\"training_data\",\"rb\"))\n",
    "        self.words=data['words']\n",
    "        self.classes=data['classes']\n",
    "        self.model=load_model('chatbot_model.h5')\n",
    "        #set the error threshold value\n",
    "        self.ERROR_THRESHOLD=0.5\n",
    "        self.ignore_words=list(\"!@#$%^&*?\")\n",
    "        \n",
    "    \n",
    "    def clean_up_sentence(self,sentence):\n",
    "        #tokenize each sentence (user's query)\n",
    "        sentence_words=word_tokenize(sentence.lower())\n",
    "        #lemmatize the word to root word and filter symbols words\n",
    "        sentence_words=list(map(lemmatizer.lemmatize,sentence_words))\n",
    "        sentence_words=list(filter(lambda x:x not in \n",
    "        self.ignore_words,sentence_words))\n",
    "        return set(sentence_words)\n",
    "\n",
    "    def wordvector(self,sentence):\n",
    "        #initialize CountVectorizer\n",
    "        #txt.split helps to tokenize single character\n",
    "        cv=CountVectorizer(tokenizer=lambda txt: txt.split())\n",
    "        sentence_words=' '.join(self.clean_up_sentence(sentence))\n",
    "        words=' '.join(self.words)\n",
    "\n",
    "        #fit the words into cv and transform into one-hot encoded vector\n",
    "        vectorize=cv.fit([words])\n",
    "        word_vector=vectorize.transform([sentence_words]).toarray().tolist()[0]\n",
    "        return(np.array(word_vector))\n",
    "    \n",
    "    def classify(self,sentence):\n",
    "        #predict to which class(tag) user's query belongs to\n",
    "        results=self.model.predict(np.array([self.wordvector(sentence)]))[0]\n",
    "        #store the class name and probability of that class\n",
    "        results = list(map(lambda x: [x[0],x[1]], enumerate(results)))\n",
    "        #accept those class probability which are greater then threshold value,0.5\n",
    "        results = list(filter(lambda x: x[1]>self.ERROR_THRESHOLD ,results))\n",
    "\n",
    "        #sort class probability value in descending order\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return_list = []\n",
    "\n",
    "        for i in results:\n",
    "            return_list.append((self.classes[i[0]],str(i[1])))\n",
    "        return return_list\n",
    "    \n",
    "    def results(self,sentence,userID):\n",
    "        #if context is maintained then filter class(tag) accordingly\n",
    "        if sentence.isdecimal():\n",
    "            if context[userID]==\"historydetails\":\n",
    "                return self.classify('ordernumber')\n",
    "        return self.classify(sentence)\n",
    "    \n",
    "    def response(self,sentence,userID='TechVidvan'):\n",
    "        #get class of users query\n",
    "        results=self.results(sentence,userID)\n",
    "        print(sentence,results)\n",
    "       \n",
    "        #store random response to the query\n",
    "        ans=\"\"\n",
    "        if results:\n",
    "            while results:\n",
    "                for i in self.intents['intents']:\n",
    "                #check if tag == query's class\n",
    "                    if i['tag'] == results[0][0]:\n",
    "\n",
    "                   #if class contains key as \"set\"\n",
    "                   #then store key as userid along with its value in\n",
    "                   #context dictionary\n",
    "                        if 'set' in i and not 'filter' in i:\n",
    "                            context[userID] = i['set']\n",
    "                    #if the tag doesn't have any filter return response\n",
    "                        if not 'filter' in i:\n",
    "                            ans=random.choice(i['responses'])\n",
    "\n",
    "                       #if a class has key as filter then check if context dictionary key's value is same as filter value\n",
    "                       #return the random response\n",
    "                        if userID in context and 'filter' in i and i['filter']==context[userID]:\n",
    "                            if 'set' in i:\n",
    "                                context[userID] = i['set']\n",
    "                        ans=random.choice(i['responses'])\n",
    "\n",
    "        results.pop(0)\n",
    "        #if ans contains some value then return response to user's query else return some message\n",
    "        return ans if ans!=\"\" else \"Sorry ! I am still Learning.\\nYou can train me by providing more datas.\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799fdd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 4ms/step - loss: 2.8148 - accuracy: 0.0682\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.8458 - accuracy: 0.0455\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.7798 - accuracy: 0.0909\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 2.7575 - accuracy: 0.1136\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.7151 - accuracy: 0.1364\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.7120 - accuracy: 0.1818\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.6277 - accuracy: 0.2500\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.6319 - accuracy: 0.1818\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.5773 - accuracy: 0.2273\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.4736 - accuracy: 0.2727\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.4797 - accuracy: 0.2273\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.4983 - accuracy: 0.1591\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.3272 - accuracy: 0.3636\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.3713 - accuracy: 0.3182\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.3099 - accuracy: 0.2500\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 2.2952 - accuracy: 0.3864\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.2767 - accuracy: 0.3864\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.2657 - accuracy: 0.3182\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 2.3442 - accuracy: 0.2955\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.1507 - accuracy: 0.3864\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.0617 - accuracy: 0.3182\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.9244 - accuracy: 0.4773\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.8893 - accuracy: 0.5455\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 1.8194 - accuracy: 0.4773\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.8008 - accuracy: 0.5455\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.9035 - accuracy: 0.4318\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.8488 - accuracy: 0.4545\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.7169 - accuracy: 0.5909\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 1.6799 - accuracy: 0.5227\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.7085 - accuracy: 0.5227\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.5334 - accuracy: 0.5682\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.4533 - accuracy: 0.5455\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.5559 - accuracy: 0.6136\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.4112 - accuracy: 0.6364\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.4562 - accuracy: 0.6136\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.2602 - accuracy: 0.6364\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.3373 - accuracy: 0.5682\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.2149 - accuracy: 0.6136\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 1.3928 - accuracy: 0.6364\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.1588 - accuracy: 0.7727\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.2626 - accuracy: 0.7045\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.1902 - accuracy: 0.7273\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.1533 - accuracy: 0.7045\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.0300 - accuracy: 0.8409\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.2724 - accuracy: 0.6364\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.9987 - accuracy: 0.7727\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.0678 - accuracy: 0.7045\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.1395 - accuracy: 0.7273\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.7541 - accuracy: 0.8636\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.0640 - accuracy: 0.6591\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.1212 - accuracy: 0.6364\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.8589 - accuracy: 0.7955\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.9007 - accuracy: 0.7727\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.8641 - accuracy: 0.7500\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8520 - accuracy: 0.7955\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.8824 - accuracy: 0.8182\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.9389 - accuracy: 0.7727\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.7755 - accuracy: 0.8409\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6582 - accuracy: 0.8636\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.7575 - accuracy: 0.7955\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.7140 - accuracy: 0.8409\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5769 - accuracy: 0.8864\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6355 - accuracy: 0.8409\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6659 - accuracy: 0.8182\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.7238 - accuracy: 0.7955\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5903 - accuracy: 0.8182\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4629 - accuracy: 0.9318\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5060 - accuracy: 0.8864\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 877us/step - loss: 0.6010 - accuracy: 0.8636\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.8172 - accuracy: 0.7500\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5420 - accuracy: 0.8409\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6334 - accuracy: 0.8864\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6260 - accuracy: 0.8636\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4661 - accuracy: 0.8636\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5102 - accuracy: 0.9091\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3869 - accuracy: 0.9545\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.4585 - accuracy: 0.9318\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.4080 - accuracy: 0.9318\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3246 - accuracy: 0.9773\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4507 - accuracy: 0.8864\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.4186 - accuracy: 0.9091\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3758 - accuracy: 0.9545\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5617 - accuracy: 0.8864\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5353 - accuracy: 0.8409\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4375 - accuracy: 0.8864\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5111 - accuracy: 0.8636\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3594 - accuracy: 0.9545\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3794 - accuracy: 0.9091\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3216 - accuracy: 0.9545\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2693 - accuracy: 0.9545\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2525 - accuracy: 0.9773\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2960 - accuracy: 0.9318\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3753 - accuracy: 0.9091\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4466 - accuracy: 0.9318\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3410 - accuracy: 0.9091\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3961 - accuracy: 0.8864\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2586 - accuracy: 0.9773\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3103 - accuracy: 0.9318\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2617 - accuracy: 0.9318\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3662 - accuracy: 0.8636\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2988 - accuracy: 0.9091\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1972 - accuracy: 0.9773\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4404 - accuracy: 0.8636\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.3417 - accuracy: 0.9091\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4544 - accuracy: 0.8636\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3803 - accuracy: 0.8864\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3145 - accuracy: 0.9091\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2764 - accuracy: 0.9091\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2765 - accuracy: 0.9545\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2785 - accuracy: 0.9091\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2842 - accuracy: 0.9318\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1774 - accuracy: 0.9545\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1563 - accuracy: 0.9773\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3616 - accuracy: 0.8864\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1520 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2695 - accuracy: 0.9545\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2291 - accuracy: 0.9773\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1790 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1806 - accuracy: 0.9545\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3182 - accuracy: 0.9545\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2299 - accuracy: 0.9091\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.3935 - accuracy: 0.8636\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2998 - accuracy: 0.9091\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1988 - accuracy: 0.9545\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2982 - accuracy: 0.8864\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1323 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2018 - accuracy: 0.9545\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2816 - accuracy: 0.9545\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1936 - accuracy: 0.9545\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1440 - accuracy: 0.9773\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2145 - accuracy: 0.9545\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2640 - accuracy: 0.9318\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1649 - accuracy: 0.9773\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.9545\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1642 - accuracy: 0.9545\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2262 - accuracy: 0.9318\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2550 - accuracy: 0.9545\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2519 - accuracy: 0.9091\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1237 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1137 - accuracy: 0.9773\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2150 - accuracy: 0.9545\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1338 - accuracy: 0.9545\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2402 - accuracy: 0.9545\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1607 - accuracy: 0.9773\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1677 - accuracy: 0.9545\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1706 - accuracy: 0.9318\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1936 - accuracy: 0.9318\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2013 - accuracy: 0.9545\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1998 - accuracy: 0.9545\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1093 - accuracy: 0.9773\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1834 - accuracy: 0.9773\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0664 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1949 - accuracy: 0.9773\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1142 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2115 - accuracy: 0.9545\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2597 - accuracy: 0.9318\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2417 - accuracy: 0.9318\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1803 - accuracy: 0.9318\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1459 - accuracy: 0.9545\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2112 - accuracy: 0.9545\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1729 - accuracy: 0.9773\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 974us/step - loss: 0.1557 - accuracy: 0.9773\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2518 - accuracy: 0.9545\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0599 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1607 - accuracy: 0.9773\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1286 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0941 - accuracy: 0.9773\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1570 - accuracy: 0.9773\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1082 - accuracy: 0.9773\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1908 - accuracy: 0.9773\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1249 - accuracy: 0.9545\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.9318\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2797 - accuracy: 0.8864\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1993 - accuracy: 0.9773\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3379 - accuracy: 0.9091\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1584 - accuracy: 0.9545\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1439 - accuracy: 0.9545\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0804 - accuracy: 0.9773\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2334 - accuracy: 0.9318\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.0820 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1531 - accuracy: 0.9545\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1991 - accuracy: 0.9318\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0956 - accuracy: 0.9773\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1069 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1706 - accuracy: 0.9773\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1101 - accuracy: 0.9545\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1092 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1593 - accuracy: 0.9545\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0544 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9318\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0946 - accuracy: 0.9773\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1359 - accuracy: 0.9545\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1557 - accuracy: 0.9545\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2465 - accuracy: 0.9318\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1620 - accuracy: 0.9545\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0767 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0761 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.0872 - accuracy: 0.9545\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "hi [('greeting', '0.9949862')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vivek\\anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\vivek\\AppData\\Local\\Temp\\ipykernel_1852\\3707768892.py\", line 74, in on_enter\n",
      "    self.bot_response(msg,\"Bot\")\n",
      "  File \"C:\\Users\\vivek\\AppData\\Local\\Temp\\ipykernel_1852\\3707768892.py\", line 79, in bot_response\n",
      "    self.text_widget.insert(END,str(sender)+\" : \"+self.test.response(msg)+\"\\n\\n\")\n",
      "  File \"C:\\Users\\vivek\\testing.py\", line 89, in response\n",
      "    if i['tag'] == results[0][0]:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "import json\n",
    "#import the training.py\n",
    "#and testing.py file\n",
    "import testing as testpy\n",
    "import training as trainpy\n",
    "BG_GRAY=\"#ABB2B9\"\n",
    "BG_COLOR=\"#000\"\n",
    "TEXT_COLOR=\"#FFF\"\n",
    "FONT=\"Helvetica 14\"\n",
    "FONT_BOLD=\"Helvetica 13 bold\"\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        #initialize tkinter window\n",
    "        self.window=Tk()\n",
    "        self.main_window()\n",
    "        self.test=testpy.Testing()\n",
    "\n",
    "#run window\n",
    "    def run(self):\n",
    "        self.window.mainloop()\n",
    "    \n",
    "    def main_window(self):\n",
    "        #add title to window and configure it\n",
    "        self.window.title(\"ChatBot\")\n",
    "        self.window.resizable(width=False,height=False)\n",
    "        self.window.configure(width=520,height=520,bg=BG_COLOR)\n",
    "        #add tab for Chatbot and Train Bot in Notebook frame\n",
    "        self.tab = ttk.Notebook(self.window)\n",
    "        self.tab.pack(expand=1,fill='both')\n",
    "        self.bot_frame=ttk.Frame(self.tab,width=520,height=520)\n",
    "        self.train_frame=ttk.Frame(self.tab,width=520,height=520)\n",
    "        self.tab.add(self.bot_frame,text='TechVidvanBot'.center(100))\n",
    "        self.tab.add(self.train_frame,text='Train Bot'.center(100))\n",
    "        self.add_bot()\n",
    "        self.add_train()\n",
    "    def add_bot(self):\n",
    "    #Add heading to the Chabot window\n",
    "        head_label=Label(self.bot_frame,bg=BG_COLOR,fg=TEXT_COLOR,text=\"Welcome to TechVidvan\",font=FONT_BOLD,pady=10)\n",
    "        head_label.place(relwidth=1)\n",
    "        line = Label(self.bot_frame,width=450,bg=BG_COLOR)\n",
    "        line.place(relwidth=1,rely=0.07,relheight=0.012)\n",
    "\n",
    "    #create text widget where conversation will be displayed\n",
    "\n",
    "        self.text_widget=Text(self.bot_frame,width=20,height=2,bg=\"#fff\",fg=\"#000\",font=FONT,padx=5,pady=5)\n",
    "        self.text_widget.place(relheight=0.745,relwidth=1,rely=0.08)\n",
    "        self.text_widget.configure(cursor=\"arrow\",state=DISABLED)\n",
    "\n",
    "        #create scrollbar\n",
    "        scrollbar=Scrollbar(self.text_widget)\n",
    "        scrollbar.place(relheight=1,relx=0.974)\n",
    "        scrollbar.configure(command=self.text_widget.yview)\n",
    "\n",
    "        #create bottom label where message widget will placed\n",
    "        bottom_label=Label(self.bot_frame,bg=BG_GRAY,height=80)\n",
    "        bottom_label.place(relwidth=1,rely=0.825)\n",
    "        #this is for user to put query\n",
    "\n",
    "        self.msg_entry=Entry(bottom_label,bg=\"#2C3E50\",fg=TEXT_COLOR,font=FONT)\n",
    "        self.msg_entry.place(relwidth=0.788,relheight=0.06,rely=0.008,relx=0.008)\n",
    "        self.msg_entry.focus()\n",
    "        self.msg_entry.bind(\"<Return>\",self.on_enter)\n",
    "        #send button which will call on_enter function to send the query\n",
    "        send_button=Button(bottom_label,text=\"Send\",font=FONT_BOLD,width=8,bg=\"Green\",command=lambda: self.on_enter(None))\n",
    "        send_button.place(relx=0.80,rely=0.008,relheight=0.06,relwidth=0.20)\n",
    "    \n",
    "    def on_enter(self,event):\n",
    "        #get user query and bot response\n",
    "        msg=self.msg_entry.get()\n",
    "        self.my_msg(msg,\"You\")\n",
    "        self.bot_response(msg,\"Bot\")\n",
    "\n",
    "    def bot_response(self,msg,sender):\n",
    "        self.text_widget.configure(state=NORMAL)\n",
    "        #get the response for the user's query from testing.py file\n",
    "        self.text_widget.insert(END,str(sender)+\" : \"+self.test.response(msg)+\"\\n\\n\")\n",
    "        self.text_widget.configure(state=DISABLED)\n",
    "        self.text_widget.see(END)\n",
    "\n",
    "    def my_msg(self,msg,sender):\n",
    "        #it will display user query and bot response in text_widget\n",
    "        if not msg:\n",
    "            return\n",
    "        self.msg_entry.delete(0,END)\n",
    "        self.text_widget.configure(state=NORMAL)\n",
    "        self.text_widget.insert(END,str(sender)+\" : \"+str(msg)+\"\\n\")\n",
    "        self.text_widget.configure(state=DISABLED)\n",
    "        \n",
    "    def on_train(self,event):\n",
    "        #read intent file and append created tag,pattern and responses from add_train function\n",
    "        with open('intents.json','r+') as json_file:\n",
    "            file_data=json.load(json_file)\n",
    "            file_data['intents'].append({\n",
    "            \"tag\": self.tag.get(),\n",
    "            \"patterns\": [i.get() for i in self.pattern],\n",
    "            \"responses\": [i.get() for i in self.response],\n",
    "            \"context\": \"\"\n",
    "            })\n",
    "            json_file.seek(0)\n",
    "            json.dump(file_data, json_file, indent = 1)\n",
    "        #run and compile model from our training.py file.\n",
    "        train=trainpy.Training()\n",
    "        train.build(); print(\"Trained Successfully\")\n",
    "        self.test=testpy.Testing()\n",
    "    \n",
    "    def add_train(self):\n",
    "        #Add heading to the Train Bot window\n",
    "        head_label=Label(self.train_frame,bg=BG_COLOR,fg=TEXT_COLOR,text=\"Train Bot\",font=FONT_BOLD,pady=10)\n",
    "        head_label.place(relwidth=1)\n",
    "\n",
    "        #Tag Label and Entry for intents tag. \n",
    "        taglabel=Label(self.train_frame,fg=\"#000\",text=\"Tag\",font=FONT)\n",
    "        taglabel.place(relwidth=0.2,rely=0.14,relx=0.008)\n",
    "        self.tag=Entry(self.train_frame,bg=\"#fff\",fg=\"#000\",font=FONT)\n",
    "        self.tag.place(relwidth=0.7,relheight=0.06,rely=0.14,relx=0.22)\n",
    "\n",
    "        #Pattern Label and Entry for pattern to our tag.\n",
    "        self.pattern=[]\n",
    "        for i in range(2):\n",
    "            patternlabel=Label(self.train_frame,fg=\"#000\",text=\"Pattern%d\"%(i+1),font=FONT)\n",
    "            patternlabel.place(relwidth=0.2,rely=0.28+0.08*i,relx=0.008)\n",
    "            self.pattern.append(Entry(self.train_frame,bg=\"#fff\",fg=\"#000\",font=FONT))\n",
    "            self.pattern[i].place(relwidth=0.7,relheight=0.06,rely=0.28+0.08*i,relx=0.22)\n",
    "\n",
    "        #Response Label and Entry for response to our pattern.\n",
    "        self.response=[]\n",
    "        for i in range(2):\n",
    "            responselabel=Label(self.train_frame,fg=\"#000\",text=\"Response%d\"%(i+1),font=FONT)\n",
    "            responselabel.place(relwidth=0.2,rely=0.50+0.08*i,relx=0.008)\n",
    "            self.response.append(Entry(self.train_frame,bg=\"#fff\",fg=\"#000\",font=FONT))\n",
    "            self.response[i].place(relwidth=0.7,relheight=0.06,rely=0.50+0.08*i,relx=0.22)\n",
    "\n",
    "        #to train our bot create Train Bot button which will call on_train function\n",
    "        bottom_label=Label(self.train_frame,bg=BG_GRAY,height=80)\n",
    "        bottom_label.place(relwidth=1,rely=0.825)\n",
    "\n",
    "        train_button=Button(bottom_label,text=\"Train Bot\",font=FONT_BOLD,width=12,bg=\"Green\",command=lambda: self.on_train(None))\n",
    "        train_button.place(relx=0.20,rely=0.008,relheight=0.06,relwidth=0.60)\n",
    "    \n",
    "\n",
    "# run the file\n",
    "if __name__==\"__main__\":\n",
    "    bot = ChatBot()\n",
    "    bot.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db0d49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 4ms/step - loss: 2.9386 - accuracy: 0.0682\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.8643 - accuracy: 0.0455\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.8049 - accuracy: 0.1136\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.7877 - accuracy: 0.0682\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.6808 - accuracy: 0.2045\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.7478 - accuracy: 0.0909\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.6857 - accuracy: 0.1136\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.6855 - accuracy: 0.1364\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.6385 - accuracy: 0.1818\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.5364 - accuracy: 0.2045\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.6257 - accuracy: 0.1591\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 2.4976 - accuracy: 0.1818\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.4621 - accuracy: 0.2273\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.4297 - accuracy: 0.1591\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.3365 - accuracy: 0.2955\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 2.3342 - accuracy: 0.2045\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.2213 - accuracy: 0.3182\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.2492 - accuracy: 0.2727\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.1874 - accuracy: 0.3636\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.1221 - accuracy: 0.3182\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.9737 - accuracy: 0.5227\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2.0301 - accuracy: 0.4091\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.9088 - accuracy: 0.3636\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 1.8310 - accuracy: 0.4318\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 1.8078 - accuracy: 0.5000\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.7486 - accuracy: 0.5000\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.9174 - accuracy: 0.4773\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 1.6022 - accuracy: 0.5227\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.7687 - accuracy: 0.4773\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.5546 - accuracy: 0.5227\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.5404 - accuracy: 0.6364\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 1.4801 - accuracy: 0.6136\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.4719 - accuracy: 0.6591\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.3540 - accuracy: 0.7500\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.3118 - accuracy: 0.6364\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.4309 - accuracy: 0.6364\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 1.1373 - accuracy: 0.6364\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.2789 - accuracy: 0.6364\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 502us/step - loss: 1.2653 - accuracy: 0.6136\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 1.2867 - accuracy: 0.5909\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.2397 - accuracy: 0.6591\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.3454 - accuracy: 0.6364\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.0601 - accuracy: 0.7727\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1438 - accuracy: 0.6591\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.9799 - accuracy: 0.7727\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.9240 - accuracy: 0.8182\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.9432 - accuracy: 0.6818\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.9691 - accuracy: 0.7045\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.9318 - accuracy: 0.8409\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.8413 - accuracy: 0.8182\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.8144 - accuracy: 0.8182\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.9229 - accuracy: 0.7727\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.8077 - accuracy: 0.8409\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.8957 - accuracy: 0.7273\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6775 - accuracy: 0.8636\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6939 - accuracy: 0.8409\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.8363 - accuracy: 0.7955\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.7268 - accuracy: 0.8409\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.9200 - accuracy: 0.6818\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.7133 - accuracy: 0.7955\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.8617 - accuracy: 0.7273\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6370 - accuracy: 0.8182\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5864 - accuracy: 0.8182\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6328 - accuracy: 0.8409\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6166 - accuracy: 0.8409\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6200 - accuracy: 0.8409\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6355 - accuracy: 0.8182\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.5471 - accuracy: 0.8864\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3718 - accuracy: 0.9773\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4328 - accuracy: 0.9091\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5053 - accuracy: 0.8409\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3704 - accuracy: 0.9318\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5460 - accuracy: 0.8636\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4551 - accuracy: 0.9545\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5110 - accuracy: 0.8864\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.5654 - accuracy: 0.8636\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3928 - accuracy: 0.8864\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.4510 - accuracy: 0.8636\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.4248 - accuracy: 0.9318\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3575 - accuracy: 0.9091\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3759 - accuracy: 0.9318\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4810 - accuracy: 0.8636\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4391 - accuracy: 0.8864\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.8864\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5217 - accuracy: 0.8409\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3721 - accuracy: 0.9091\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3707 - accuracy: 0.9091\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.8636\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4116 - accuracy: 0.8636\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3772 - accuracy: 0.9091\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4284 - accuracy: 0.9545\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.4858 - accuracy: 0.8409\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4185 - accuracy: 0.8636\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.3536 - accuracy: 0.9545\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2821 - accuracy: 0.9318\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3125 - accuracy: 0.9545\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3571 - accuracy: 0.9318\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2887 - accuracy: 0.9545\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2720 - accuracy: 0.9318\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3237 - accuracy: 0.8864\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2165 - accuracy: 0.9773\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4527 - accuracy: 0.8409\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2885 - accuracy: 0.9091\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1908 - accuracy: 0.9545\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.7955\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2893 - accuracy: 0.9318\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.3735 - accuracy: 0.9091\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.3394 - accuracy: 0.8864\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2294 - accuracy: 0.9773\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.3033 - accuracy: 0.8864\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.9318\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.3010 - accuracy: 0.9318\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.3189 - accuracy: 0.9318\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1244 - accuracy: 0.9773\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2851 - accuracy: 0.9773\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2398 - accuracy: 0.9318\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1842 - accuracy: 0.9773\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2478 - accuracy: 0.9545\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3901 - accuracy: 0.9091\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2701 - accuracy: 0.9545\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2407 - accuracy: 0.9545\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2207 - accuracy: 0.9773\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2438 - accuracy: 0.9091\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2877 - accuracy: 0.9318\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2553 - accuracy: 0.9318\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2729 - accuracy: 0.9545\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2553 - accuracy: 0.9545\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1742 - accuracy: 0.9773\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4244 - accuracy: 0.8409\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3567 - accuracy: 0.8864\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1472 - accuracy: 0.9545\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2873 - accuracy: 0.9545\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1677 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2391 - accuracy: 0.9773\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1994 - accuracy: 0.9773\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1089 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1063 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1314 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0936 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2546 - accuracy: 0.9545\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2531 - accuracy: 0.9545\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1753 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1558 - accuracy: 0.9318\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2116 - accuracy: 0.9318\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1150 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1363 - accuracy: 0.9545\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2136 - accuracy: 0.9545\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1000 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1504 - accuracy: 0.9773\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2198 - accuracy: 0.9318\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1352 - accuracy: 0.9773\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1470 - accuracy: 0.9773\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1337 - accuracy: 0.9773\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1275 - accuracy: 0.9773\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1755 - accuracy: 0.9545\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1899 - accuracy: 0.9545\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1977 - accuracy: 0.9318\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1387 - accuracy: 0.9773\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1981 - accuracy: 0.9318\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1028 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2723 - accuracy: 0.8864\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.2280 - accuracy: 0.9091\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1174 - accuracy: 0.9545\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1393 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1547 - accuracy: 0.9545\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1441 - accuracy: 0.9545\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0876 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1346 - accuracy: 0.9773\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1291 - accuracy: 0.9545\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1331 - accuracy: 0.9545\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.0556 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1689 - accuracy: 0.9545\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1407 - accuracy: 0.9773\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1033 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1569 - accuracy: 0.9545\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1073 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1608 - accuracy: 0.9545\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0944 - accuracy: 0.9773\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1713 - accuracy: 0.9545\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0935 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1281 - accuracy: 0.9773\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1126 - accuracy: 0.9773\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1570 - accuracy: 0.9773\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.0596 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1083 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1415 - accuracy: 0.9773\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1343 - accuracy: 0.9773\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.9545\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1765 - accuracy: 0.9545\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1748 - accuracy: 0.9545\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1497 - accuracy: 0.9545\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1113 - accuracy: 0.9773\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0566 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1055 - accuracy: 0.9773\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1018 - accuracy: 0.9773\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.1201 - accuracy: 0.9773\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 0s/step - loss: 0.0378 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0883 - accuracy: 0.9773\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1134 - accuracy: 0.9545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 354ms/step\n",
      "HI [('greeting', '0.9982949')]\n",
      "Query: HI\n",
      "Bot: Hi , how can I help?\n",
      "1/1 [==============================] - 0s 238ms/step\n",
      "HI [('greeting', '0.9982949')]\n",
      "Query: HI\n",
      "Bot: Hi , how can I help?\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "HOW ARE YOU [('botstatus', '0.9958192')]\n",
      "Query: HOW ARE YOU\n",
      "Bot: I am doing good.\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "HOW ARE YOU [('botstatus', '0.9958192')]\n",
      "Query: HOW ARE YOU\n",
      "Bot: I am doing good.\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "WHAT IS YOUR NAME [('creator', '0.9999639')]\n",
      "Query: WHAT IS YOUR NAME\n",
      "Bot: Hi my name is Maya. 'TechVantage' team has created me.\n",
      "Nice to meet you.\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "WHAT IS YOUR NAME [('creator', '0.9999639')]\n",
      "Query: WHAT IS YOUR NAME\n",
      "Bot: Hi my name is Maya. 'TechVantage' team has created me.\n",
      "Nice to meet you.\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "WHAT IS THE WEATHER TODAY [('Weather', '0.99986863')]\n",
      "Query: WHAT IS THE WEATHER TODAY\n",
      "Bot: Its sunny outside\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "WHAT IS THE WEATHER TODAY [('Weather', '0.99986863')]\n",
      "Query: WHAT IS THE WEATHER TODAY\n",
      "Bot: No it isnt raining today\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "YOU ARE SMART []\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WHERE IS LONDON []\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "WHERE IS DELHI []\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "hI [('greeting', '0.9982949')]\n",
      "Query: hI\n",
      "Bot: Hello , thanks for visiting TechVantage AI Lab\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "hI [('greeting', '0.9982949')]\n",
      "Query: hI\n",
      "Bot: Good to see you again! \n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "Hi [('greeting', '0.9982949')]\n",
      "Query: Hi\n",
      "Bot: Hi , how can I help?\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Hi [('greeting', '0.9982949')]\n",
      "Query: Hi\n",
      "Bot: Hello , thanks for visiting TechVantage AI Lab\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "How are you [('botstatus', '0.9958192')]\n",
      "Query: How are you\n",
      "Bot: I am fine.Thanks for asking.\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "How are you [('botstatus', '0.9958192')]\n",
      "Query: How are you\n",
      "Bot: I am fine.Thanks for asking.\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Where are you []\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Hi [('greeting', '0.9982949')]\n",
      "Query: Hi\n",
      "Bot: Good to see you again! \n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "Hi [('greeting', '0.9982949')]\n",
      "Query: Hi\n",
      "Bot: Hi , how can I help?\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "How are you [('botstatus', '0.9958192')]\n",
      "Query: How are you\n",
      "Bot: I am doing good.\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "How are you [('botstatus', '0.9958192')]\n",
      "Query: How are you\n",
      "Bot: I am doing good.\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "What is your name [('creator', '0.9999639')]\n",
      "Query: What is your name\n",
      "Bot: Hi my name is Maya. 'TechVantage' team has created me.\n",
      "Nice to meet you.\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "What is your name [('creator', '0.9999639')]\n",
      "Query: What is your name\n",
      "Bot: Hi my name is Maya. 'TechVantage' team has created me.\n",
      "Nice to meet you.\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Is it raining today [('Weather', '0.9996903')]\n",
      "Query: Is it raining today\n",
      "Bot: No it isnt raining today\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Is it raining today [('Weather', '0.9996903')]\n",
      "Query: Is it raining today\n",
      "Bot: Its sunny outside\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Where is delhi []\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Where is delhi []\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Hi [('greeting', '0.9982949')]\n",
      "Query: Hi\n",
      "Bot: Good to see you again! \n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Hi [('greeting', '0.9982949')]\n",
      "Query: Hi\n",
      "Bot: Hello , thanks for visiting TechVantage AI Lab\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "How are you [('botstatus', '0.9958192')]\n",
      "Query: How are you\n",
      "Bot: I am fine.Thanks for asking.\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "How are you [('botstatus', '0.9958192')]\n",
      "Query: How are you\n",
      "Bot: I am fine.Thanks for asking.\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "What is your name [('creator', '0.9999639')]\n",
      "Query: What is your name\n",
      "Bot: Hi my name is Maya. 'TechVantage' team has created me.\n",
      "Nice to meet you.\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "What is your name [('creator', '0.9999639')]\n",
      "Query: What is your name\n",
      "Bot: Hi my name is Maya. 'TechVantage' team has created me.\n",
      "Nice to meet you.\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Are you a machine []\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Are you sentient []\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "What is your age [('creator', '0.99873')]\n",
      "Query: What is your age\n",
      "Bot: Hi my name is Maya. 'TechVantage' team has created me.\n",
      "Nice to meet you.\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "What is your age [('creator', '0.99873')]\n",
      "Query: What is your age\n",
      "Bot: Hi my name is Maya. 'TechVantage' team has created me.\n",
      "Nice to meet you.\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Are you male []\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Do you have any crush []\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "What is ex []\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "define sunny []\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Define sex []\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "What is a bot []\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "are you a robot []\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "how many wheels do you have []\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "do you have legs []\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "can you have food []\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "import json\n",
    "#import the training.py\n",
    "#and testing.py file\n",
    "import testing as testpy\n",
    "import training as trainpy\n",
    "import transformers\n",
    "nlp = transformers.pipeline(\"conversational\", \n",
    "                            model=\"microsoft/DialoGPT-large\")\n",
    "import numpy as np\n",
    "\n",
    "BG_GRAY=\"#ABB2B9\"\n",
    "BG_COLOR=\"#000\"\n",
    "TEXT_COLOR=\"#FFF\"\n",
    "FONT=\"Helvetica 14\"\n",
    "FONT_BOLD=\"Helvetica 13 bold\"\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        #initialize tkinter window\n",
    "        self.window=Tk()\n",
    "        self.main_window()\n",
    "        self.test=testpy.Testing()\n",
    "        \n",
    "    #run window\n",
    "    def run(self):\n",
    "        self.window.mainloop()\n",
    "    \n",
    "    def main_window(self):\n",
    "        #add title to window and configure it\n",
    "        self.window.title(\"ChatBot\")\n",
    "        self.window.resizable(width=False,height=False)\n",
    "        self.window.configure(width=520,height=520,bg=BG_COLOR)\n",
    "        #add tab for Chatbot and Train Bot in Notebook frame\n",
    "        self.tab = ttk.Notebook(self.window)\n",
    "        self.tab.pack(expand=1,fill='both')\n",
    "        self.bot_frame=ttk.Frame(self.tab,width=520,height=520)\n",
    "        self.train_frame=ttk.Frame(self.tab,width=520,height=520)\n",
    "        self.tab.add(self.bot_frame,text='TechVantageBot'.center(100))\n",
    "        self.tab.add(self.train_frame,text='Train Bot'.center(100))\n",
    "        self.add_bot()\n",
    "        self.add_train()\n",
    "        \n",
    "    def add_bot(self):\n",
    "        #Add heading to the Chabot window\n",
    "        head_label=Label(self.bot_frame,bg=BG_COLOR,fg=TEXT_COLOR,text=\"Welcome to TechVantage\",font=FONT_BOLD,pady=10)\n",
    "        head_label.place(relwidth=1)\n",
    "        line = Label(self.bot_frame,width=450,bg=BG_COLOR)\n",
    "        line.place(relwidth=1,rely=0.07,relheight=0.012)\n",
    "\n",
    "        #create text widget where conversation will be displayed\n",
    "        self.text_widget=Text(self.bot_frame,width=20,height=2,bg=\"#fff\",fg=\"#000\",font=FONT,padx=5,pady=5)\n",
    "        self.text_widget.place(relheight=0.745,relwidth=1,rely=0.08)\n",
    "        self.text_widget.configure(cursor=\"arrow\",state=DISABLED)\n",
    "\n",
    "        #create scrollbar\n",
    "        scrollbar=Scrollbar(self.text_widget)\n",
    "        scrollbar.place(relheight=1,relx=0.974)\n",
    "        scrollbar.configure(command=self.text_widget.yview)\n",
    "\n",
    "        #create bottom label where message widget will placed\n",
    "        bottom_label=Label(self.bot_frame,bg=BG_GRAY,height=80)\n",
    "        bottom_label.place(relwidth=1,rely=0.825)\n",
    "        #this is for user to put query\n",
    "        self.msg_entry=Entry(bottom_label,bg=\"#2C3E50\",fg=TEXT_COLOR,font=FONT)\n",
    "        self.msg_entry.place(relwidth=0.788,relheight=0.06,rely=0.008,relx=0.008)\n",
    "        self.msg_entry.focus()\n",
    "        self.msg_entry.bind(\"<Return>\",self.on_enter)\n",
    "        #send button which will call on_enter function to send the query\n",
    "        send_button=Button(bottom_label,text=\"Send\",font=FONT_BOLD,width=8,bg=\"Green\",command=lambda: self.on_enter(None))   \n",
    "        send_button.place(relx=0.80,rely=0.008,relheight=0.06,relwidth=0.20)\n",
    "\n",
    "    def add_train(self):\n",
    "        #Add heading to the Train Bot window\n",
    "        head_label=Label(self.train_frame,bg=BG_COLOR,fg=TEXT_COLOR,text=\"Train Bot\",font=FONT_BOLD,pady=10)\n",
    "        head_label.place(relwidth=1)\n",
    "\n",
    "        #Tag Label and Entry for intents tag. \n",
    "        taglabel=Label(self.train_frame,fg=\"#000\",text=\"Tag\",font=FONT)\n",
    "        taglabel.place(relwidth=0.2,rely=0.14,relx=0.008)\n",
    "        self.tag=Entry(self.train_frame,bg=\"#fff\",fg=\"#000\",font=FONT)\n",
    "        self.tag.place(relwidth=0.7,relheight=0.06,rely=0.14,relx=0.22)\n",
    "\n",
    "        #Pattern Label and Entry for pattern to our tag.\n",
    "        self.pattern=[]\n",
    "        for i in range(2):\n",
    "            patternlabel=Label(self.train_frame,fg=\"#000\",text=\"Pattern%d\"%(i+1),font=FONT)\n",
    "            patternlabel.place(relwidth=0.2,rely=0.28+0.08*i,relx=0.008)\n",
    "            self.pattern.append(Entry(self.train_frame,bg=\"#fff\",fg=\"#000\",font=FONT))\n",
    "            self.pattern[i].place(relwidth=0.7,relheight=0.06,rely=0.28+0.08*i,relx=0.22)\n",
    "\n",
    "        #Response Label and Entry for response to our pattern.\n",
    "        self.response=[]\n",
    "        for i in range(2):\n",
    "            responselabel=Label(self.train_frame,fg=\"#000\",text=\"Response%d\"%(i+1),font=FONT)\n",
    "            responselabel.place(relwidth=0.2,rely=0.50+0.08*i,relx=0.008)\n",
    "            self.response.append(Entry(self.train_frame,bg=\"#fff\",fg=\"#000\",font=FONT))\n",
    "            self.response[i].place(relwidth=0.7,relheight=0.06,rely=0.50+0.08*i,relx=0.22)\n",
    "\n",
    "        #to train our bot create Train Bot button which will call on_train function\n",
    "        bottom_label=Label(self.train_frame,bg=BG_GRAY,height=80)\n",
    "        bottom_label.place(relwidth=1,rely=0.825)\n",
    "\n",
    "        train_button=Button(bottom_label,text=\"Train Bot\",font=FONT_BOLD,width=12,bg=\"Green\",command=lambda: self.on_train(None))\n",
    "        train_button.place(relx=0.20,rely=0.008,relheight=0.06,relwidth=0.60)\n",
    "    \n",
    "    def on_train(self,event):\n",
    "        #read intent file and append created tag,pattern and responses from add_train function\n",
    "        with open('intents.json','r+') as json_file:\n",
    "            file_data=json.load(json_file)\n",
    "            file_data['intents'].append({\n",
    "            \"tag\": self.tag.get(),\n",
    "            \"patterns\": [i.get() for i in self.pattern],\n",
    "            \"responses\": [i.get() for i in self.response],\n",
    "            \"context\": \"\"\n",
    "            })\n",
    "            json_file.seek(0)\n",
    "            json.dump(file_data, json_file, indent = 1)\n",
    "        #run and compile model from our training.py file.\n",
    "        train=trainpy.Training()\n",
    "        train.build(); print(\"Trained Successfully\")\n",
    "        self.test=testpy.Testing()\n",
    "        \n",
    "    def on_enter(self,event):\n",
    "        #get user query and bot response\n",
    "        msg=self.msg_entry.get()\n",
    "        self.my_msg(msg,\"You\")\n",
    "        self.bot_response(msg,\"Bot\")\n",
    "        \n",
    "    def bot_response(self,msg,sender):\n",
    "        self.text_widget.configure(state=NORMAL)\n",
    "        #get the response for the user's query from testing.py file\n",
    "        a = self.test.response(msg)\n",
    "        if a != \"\":\n",
    "            self.text_widget.insert(END,str(sender)+\" : \"+self.test.response(msg)+\"\\n\\n\")\n",
    "            self.text_widget.configure(state=DISABLED)\n",
    "            self.text_widget.see(END)\n",
    "        else:\n",
    "            chat = nlp(transformers.Conversation(msg), pad_token_id=50256)\n",
    "            res = str(chat)\n",
    "            res = res[res.find(\"bot >> \")+6:].strip()\n",
    "            self.text_widget.insert(END,str(sender)+\" : \"+res+\"\\n\\n\")\n",
    "            self.text_widget.configure(state=DISABLED)\n",
    "            self.text_widget.see(END)\n",
    "    \n",
    "    def my_msg(self,msg,sender):\n",
    "        #it will display user query and bot response in text_widget\n",
    "        if not msg:\n",
    "            return\n",
    "        self.msg_entry.delete(0,END)\n",
    "        self.text_widget.configure(state=NORMAL)\n",
    "        self.text_widget.insert(END,str(sender)+\" : \"+str(msg)+\"\\n\")\n",
    "        self.text_widget.configure(state=DISABLED)\n",
    "        \n",
    "# run the file\n",
    "if __name__==\"__main__\":\n",
    "    bot = ChatBot()\n",
    "    bot.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfe937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221b050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ca2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
